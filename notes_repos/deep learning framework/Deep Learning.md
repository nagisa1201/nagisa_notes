#深度学习从零开始 #框架式的全面学习
>**该笔记是Nagisa在学习[《动手学深度学习》](https://zh.d2l.ai/)时的学习笔记，笔者认为，学习深度学习，是需要从零开始对神经网络、机器学习的一个总体框架和认识，本文笔记会十分的长，用作自己复习时使用，希望也可以帮到看这篇笔记的你！**

# 数学基础
## 数据操作基础
- 见代码仓库中的data_operation.ipynb文件及其注释
# ***穿插重点：LaTex数学公式***
由于笔记后将会出现大量的数学公式，笔者不希望通过截图让笔记变得过于冗余和格式错误，故自学了LaTex的数学公式排版。
## 首先附LaTex数学公式表
![[Pasted image 20250731171727.png]]
### 上下标
- 对于**单个变量（字符）**而言，直接加^表示上标（shift+6），加_表示下标
- 对于**多个字符而言**，需要加{}将都要上标或者下标的内容框起来才可以正常渲染
### 直立体和斜体
![[Pasted image 20250731172843.png]]
以一个例子为例：
![[Pasted image 20250731172931.png]]
- $x_{i}$：可以直接通过x_i打出，默认斜体，表示的是变量
- $x_{\text{i}}$：**需要通过x_{\rm i}(\rm表示roman罗马体)或者x_{\text i}**
>*Q:罗马体和文本又有什么区别呢？*
>*A:\text{A B}可显示空格A B，但\rm{A B}直接显示渲染出的AB*，**且\text A B只会对A生效，而\rm A B会对两者生效**
### 分式和根式
- **分式的嵌套**$\frac{{\frac{1}{x+1}+3}}{y+1}$
- frac{{\\***（d）***frac{1}{x+1}+3}}{y+1}使用这样的方式，且可以在图中地方加上***d***，可以将那个部分的分式变大，如图所示$\frac{{{\dfrac{1}{x+1}+3}}}{y+13}$
- 根式则是\sqrt[114514]{ 0721+00x }即$\sqrt[114514]{ 0721+00x }$
### 普通运算符
- +，-同理
- \pm,\mp对应$\pm$，$\mp$
- 叉乘、点乘、除分别对应$\times$，$\cdot$，$\div$
- ><同理
- 大于等于和小于等于，远大于远小于，不等于、约等于、恒等于对应$\ge$,$\le$，$\gg$，$\ll$，$\ne$，$\approx$，$\equiv$
- 集合运算符$\cap$，$\cup$，$\in$，$\not\in$，$\subseteq$，$\subsetneqq$，$\varnothing$
- 逻辑表达式$\forall$，$\exists$，$\nexists$，$\because$，$\therefore$
- $\mathcal F$，$\mathscr F$傅里叶变换，$\mathscr L$拉普拉斯变换
- $\infty$，$\partial$，$\nabla$，$\degree$，$\propto$
- $\sin x$，$\cos x$，$\sec x$等三角函数
- $\log x$，$\ln x$，$\lim_{x \to 0} \dfrac {x}{\sin x}$(有的不会显示可以改成$\lim\limits_{x \to 0} \dfrac {x}{\sin x}$）
- 直体符号$\text{MSE}(x)$
### 数集类
- $Q$，$R$，$N$，$Z_{+}$，$\mathbb R$，$\mathbb Q$
### 大型运算符
- $\sum$，$\prod$
- 加上限定修饰$\sum\limits_{i=0}^{n}$，$\dfrac{\sum\limits_{i=0}^N isini}{\prod\limits_{i=1}^kx_{i}}$
- 积分符号$\int$，$\iint$，$\iiint$，$\oint$，$\oiint$（此处是二次回路积分不知道为什么渲染不出来）
- 上下限的积分$\int_{-\infty}^{\infty}f(x)dx$，此后为更严谨的写法$\int_{-\infty}^{\infty}f(x)\,\text d x$(\\,和\\ 是LaTex规定的空出小间距，而\\quad，\\qquad则是更大间距)
### 标注符号
![[Pasted image 20250731212954.png]]
以及补充的下方：
![[Pasted image 20250731212943.png]]
### 箭头 
![[Pasted image 20250731213127.png]]
### 括号与界定符
![[Pasted image 20250731214213.png]]
>***对于第四行的解释：我们希望这个|能够自适应长度，但是没有对应的括号与之对应，因此我们在开头用\left.虚拟一个左括号然后把|当做右括号就可以自适应其大小了***
### 多行公式与大括号
![[Pasted image 20250731215837.png]]
>***注意：&符号的意思是前后的&后的东西会对齐***
![[Pasted image 20250731220039.png]]
### 矩阵
![[Pasted image 20250731220229.png]]
## 对于Obsidian的Latex Suite插件而言的笔记：与Latex略有区别，但基本类似
附官方仓库表格：
![[Pasted image 20250731172225.png]]
![[Pasted image 20250731172255.png]]
- 其中，表格一是**排版，算符等的代替**，表格二是**希腊字母**的所有代替
# 线性神经网络
## 基本元素：线性模型的根本
### 损失函数loss function：量化目标实际值和预测值之间的差距
- 常用损失函数为平方误差函数，样本 $di$的预测值为$\hat{y}^{(i)}$,其真实标签为$y^{(i)}$时，平方误差为$$l^{(i)}(w,b)=\dfrac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2$$
- 这个公式表明，当预测值（估计值）和真实值（观测值）差距越大，会导致更大的损失。因而***为了度量模型在整个数据集的预测质量，取训练集n个样本上的损失均值，即***$$\dfrac {1}{n} \sum\limits_{i=1}^n l^{(i)}(w,b)$$
- 所以训练模型的意义就在于寻找**使得总损失的最小值的参数$w，b$**
- 平方损失函数和正态分布密切，均方误差函数可以用于线性回归的原因是，***假设观测中存在噪声，噪声服从正态分布，估计参数$w,b$的过程，即一个极大似然估计过程（以防概率论基础遗忘，极大似然估计是在随数据测量增大，估计的结果趋近于真实值基础上的，即观测量依概率收敛于真实量）***
![[Pasted image 20250801112024.png]]


### 解析解
- 由来：线性回归是一个简单的优化问题，其损失函数并不复杂，**解析解的本质就是通过​​求导并令导数为零​​来求解最小化问题的参数值，是通过代数方法直接给出优化问题的全局最优解**
- 也正是因为线性回归当损失函数满足于一定要求时才存在解析解
![[Pasted image 20250801102701.png]]
### 随机梯度下降gradient descent
#### 由来和思路
- 梯度下降是用于在无法得到解析解的方法下有效训练模型的方式，*在许多难以优化的模型中，梯度下降的方法优化效果会更好，***梯度下降几乎可以优化所有的模型**
- 梯度下降最简单方法：计算损失函数（数据集的样本损失均值）关于模型参数导数（梯度）
>当数据集的数量庞大时，每一步都需要遍历数据集，效率过低。***所以在每次需要计算更新时随机抽取一小批样本$B$，称小批量梯度下降***
>**在每次迭代中，先抽取小批量$B$，计算其损失均值关于模型参数导数（也称梯度），之后将这一梯度$×\eta$（一个预先确定正数）并从当前参数中减去**
- 我们可以直接给出公式
![[Pasted image 20250801104230.png]]
#### ***重点：公式与参数辨析***
![[Pasted image 20250801104638.png]]
**注意好批量大小，学习率，超参数和调参的概念辨析，知道批量大小和学习率通常是固定值，更新只在训练完之后才会更新，单次训练中不会更新**
## 线性回归的不足
- 线性回归是在整个域中只存在一个最小值的学习问题，但是深度神经网络复杂模型，损失平面上通常有多个最小值，在训练集上的损失达到最小值的参数组寻找本身足够困难，更难的事找到一个参数组，使其在从未见过的数据集上实现小损失，即***泛化generalization***的困难
## 向量化加速
- 见代码仓库[向量化加速](https://github.com/nagisa1201/nagisa_notes/blob/main/code_repos/2.linear_nn/vector_speed.ipynb)
### 线性回归到深度网络
- **线性回归是一个单层神经网络**
- 输入层中的输入数或者特征维度feature dimensionality为d
- 输出层中的输入数为1
- **由于模型重点在发生计算的地方，计算层数时一般不考虑输入层，所以如图，线性回归网络看做单层神经网络**
![[Pasted image 20250801113752.png]]
- ***概念重点：每个输入都与每个输出（该例子只有一个输出）连接，这种变换称为全连接层，也称稠密层***

# Softmax回归（也属于线性神经网络）：分类问题的解决
## 起源和发展
### 独热编码one-hot encoding
- 独热编码是一个向量，他的分量和类别一样多，类别对应的分量设置为1，其他所有分量设置为0。 
- 举例：从一个图像分类问题开始。 假设每次输入是一个的灰度图像。 我们可以用一个标量表示每个像素值，每个图像对应四个特征。 此外，假设每个图像属于类别“猫”“鸡”和“狗”中的一个。
- 在我们的例子中，标签将是一个三维向量， 其中$(1,0,0)$对应于“猫”、$(0,1,0)$对应于“鸡”、$(0,0,1)$对应于“狗”
### 网络架构
为了估计所有可能类别的条件概率，需要一个有多个输出的模型，每个类别对应一个输出。 为了解决线性模型的分类问题，我们需要和输出一样多的**仿射函数**affine function。 每个输出对应于它自己的仿射函数
- 此例中有4个特征3个可能输出类别，故有如下的预测，并用神经网络表示计算过程
 ![[Pasted image 20250801174522.png]]
 - 用线性代数简洁表示，$o$代表预测量矩阵，$W$代表参数矩阵，$x$代表输入矩阵，$b$代表偏置量可得$$o=Wx+b$$
 - ***全连接层的参数开销是巨大的：全连接层是“完全”连接的，可能有很多可学习的参数。 具体来说，对于任何具有$d$个输入和$q$个输出的全连接层， 参数开销为$O(dq)$，这个数字在实践中可能高得令人望而却步。 幸运的是，将如上的输入输出的转化成本可以减少到$O(\dfrac {dq}{n})$， 其中超参数$n$可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性***
## Softmax运算
- **因为分类问题，我们需要计算一个输入$x_{i}$对应输出的$\hat{y}_{j}$为视作属于类$j$的概率，然后输出具有最大输出值的类别$argmax_{j}y_{j}$作为我们的实际预测结果**
- 但是由于概率论的公理，没有限制这些输出的概率总和为1，并且每个输出不可为负
- 必须保证在任何数据上的输出都是非负的且总和为1。 我们需要一个训练的目标函数，来激励模型精准地估计概率。 例如在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别。 这个属性叫做**校准calibration**
- **Softmax函数可以将能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质**
- **我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。**$$\hat{y}=\text{softmax}(o)\quad其中\quad \hat{y}_{j}=\dfrac{exp(o_{j})}{\sum_{k}\exp(o_{k})} ①$$
- 这样，对于所有的j总有$0\le\hat{y}_{j}\le{1}$，这样可以得到正确的概率分布了，并且这个函数不会改变大小次序，因而仍可以取最大值来获得最可能的类别
***尽管softmax是一个非线性函数，但其输出让然可以有输入仿射变换决定，因此Softmax回归是一个线性模型***
## 损失函数
在softmax回归中，我们需要一个损失函数来度量预测结果。我们使用了***最大似然估计***
### 对数似然
softmax函数给出了向量$\hat{y}$，他代表**输入一个$x$的每个类别的条件概率**，假设整个数据集$\{X,Y\}$有$n$个样本，其中索引$i$的样本由特征向量$x^{(i)}$（表示输入的多个特征的数据）和独热标签向量$y^{(i)}$组成，我们可以将估计与实际值比较
![[Pasted image 20250801192006.png]]
该函数被称为***交叉熵损失***

### softmax及其导数
将上述softmax①式带入到（3.4.8）式中可得
![[Pasted image 20250801192455.png]]
***导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。 从这个意义上讲，这与我们在回归中看到的非常相似， 其中梯度是观测值和估计值之间的差异。***
- 这并非巧合，而是在任何指数族分布模型中，对数似然的梯度正是由此得出。
### 信息论基础和交叉熵损失
***​​Softmax的损失函数被称为交叉熵损失的核心原因​​，正是因为通过最大似然估计（负对数似然）推导出的优化目标与交叉熵在数学形式上完全一致。***
- 给出信息论中熵entropy的方程，可见形式完全一致$$H[P]=\sum\limits_{j}-P(j)\log P(j)$$
- ***可以理解为，softmax将输入的特征向量转化输出为多维标签向量的类别概率分布向量，描述预测不确定性；而独热编码是离散的确定性标签***
- ***而在训练时需要与独热标签向量对比，计算交叉熵损失***

# 多层感知机
## 隐藏层
线性模型常常会出错，预处理无法将所有模型处理为线性模型
- 我们可以通过在网络中**加入一个或多个隐藏层**来克服线性模型的限制， 使其能处理更普遍的函数关系类型
- 最简单的方法是将许多全连接层堆叠在一起，每一层输出到上面的层，直到生成最后输出把前$L-1$层看作表示，把最后一层看作线性预测器，这样的架构通常称为***多层感知机multilayer perceptron(MLP)***
![[Pasted image 20250801212108.png]]
![[Pasted image 20250801212127.png]]
- **挑战与问题：具有全连接层的多层感知机的参数开销可能会高的吓人，需要在参数节约和模型有效性之间权衡**
## 从线性到非线性
![[Pasted image 20250801212419.png]]
>说明：上式代表输出层到隐藏层的线性变换关系，下式代表隐藏层到输出层之间的线性变换关系
- 需要注意的是：从上面的操作而言，我们增加了需要更新和跟踪的参数，但没有得到**任何好处**，上面的隐藏单元由输入的仿射函数给出，而输出之前也经过仿射函数，***线性函数的线性函数仍然是线性函数（线性改为仿射同理），但之前的线性函数已经可以表示***，可以证明
![[Pasted image 20250801213049.png]]
### 激活函数：发挥多层架构优势
- 在仿射变换后对每个隐藏单元应用***非线性激活函数***$\sigma$，激活函数的输出被称为***活性值***，可以防止多层感知机***退化为线性模型***$$H=\sigma(XW^{(1)}+b^{(1)})$$$$O=HW^{(2)}+b^{(2)}$$
- 为了构建更通用的多层感知机， 我们可以继续**堆叠这样的隐藏层**$$H=\sigma_1(XW^{(1)}+b^{(1)})$$$$H=\sigma_{2}(XW^{(2)}+b^{(2)})$$
-  通用近似定理：**通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数**
#### ReLU函数Rectified Linear unit：修正线性单元
实现简单，且在各种预测任务表现良好，是非常简单的非线性变换，对于给定的元素$x$：$$\text{ReLU}(x)=\text{max}(x,0)$$仅保留正元素并丢弃所有负元素
- **使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题**
- 存在很多变体：如参数化pReLU，该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过$$\text{pReLU}(x)=\text{max}(x,0)+\alpha\,\text{min}(0,x)$$
#### sigmoid函数squashing function：
对于定义域$R$上的输入，该函数可以将输入变换到$(0,1)$上的输出，故也称作**挤压函数**，将$(-\infty,\infty)$任意输出压缩到$(0,1)$上某个值$$\text{sigmoid}(x)=\dfrac{1}{1+e^{-x}}$$
- 该函数在隐藏层中已经较少使用，大部分时候被**更简单和容易训练的ReLU**所替代
![[Pasted image 20250802173923.png]]
![[Pasted image 20250802173934.png]]
#### tanh函数
与sigmoid函数类似，双曲正切函数可以将$(-\infty,\infty)$任意输出压缩到$(-1,1)$上某个值$$\text{tanh}(x)=\dfrac{1-e^{-2x}}{1+e^{-2x}}$$
![[Pasted image 20250802174145.png]]
![[Pasted image 20250802174151.png]]